{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c212a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This script will be used to dissagregate the volume give either by our commercial intelligence or our own forecast model.\n",
    "There are 3 types of levels to this dissagregation. Abs_type = 1 which takes in the general volume of the month. Abs_type = 0\n",
    "gives us a dataframe from excel with the volume of each city. Abs_type = -1 gives us a dataframe from excel with \n",
    "the volume of each factory.\n",
    "\"\"\"\n",
    "#import libraries\n",
    "\n",
    "import pandas as pd # dataframe library\n",
    "import numpy as np #mathematical library\n",
    "import datetime #date library\n",
    "from dateutil.relativedelta import relativedelta #subtract periods to a date\n",
    "import sys #system exit\n",
    "import time\n",
    "\n",
    "#import our sql script to connecto to engine and return dataframe. In this case, \n",
    "#the %run is used to \"import\" our sql connection notebook\n",
    "\n",
    "%run ..\\sql\\connect_sql_server.ipynb\n",
    "#from ipynb.fs.full.connect_sql_server import querySQL --> this is another option to import another notebook when in the same folder\n",
    "\n",
    "#import warnings library to then avoid the warnings given by jupyter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c6f07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to send dataframe to excel\n",
    "def send_excel(df,country):\n",
    "    #get current datetime\n",
    "    now = pd.to_datetime(\"now\").strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    #create excel worksheet\n",
    "    create_excel = pd.ExcelWriter(\"../data/\"+country+\"/Desagregacion_\" + now + \".xlsx\", engine='xlsxwriter') #create excel to save dataframe\n",
    "    df.to_excel( create_excel, sheet_name=\"Desagregado\", index=False ) #send dataframe day to excel sheet created previously\n",
    "    create_excel.save() #save the workbook\n",
    "    \n",
    "#we need to create a dataframe to get the stational factors for each week, day of the week and factory\n",
    "def get_stational_factors(df_history,country,year_disaggregate,month_disaggregate,inactivate_temp,calendar_sql,volume,abs_type,active_factories):\n",
    "    \n",
    "    \n",
    "    #first organize the historic dataframe by date\n",
    "    #convert string date to datetime type\n",
    "    df_history['FechaEntrega'] = pd.to_datetime(df_history['FechaEntrega'])\n",
    "    df_history = df_history.sort_values(by=\"FechaEntrega\")\n",
    "    #filter out inactive factories\n",
    "    df_history = df_history[df_history.Planta.isin(active_factories.Centro)]\n",
    "    if len(inactivate_temp) > 0:\n",
    "            df_history = df_history[~df_history.Planta.isin(inactivate_temp)]\n",
    " \n",
    "        #--------------------------------------get stational factors by FACTORY-------------------------------------------\n",
    "        \n",
    "    #the abs_type variable will define how we distribute the forecast volume by each factory.\n",
    "    #sort our dataframe by date\n",
    "    if abs_type == 1:\n",
    "        #groupby factory with the sum of total volume\n",
    "        df_stational_factor_factory = df_history.groupby(['Planta'])['totalEntregado'].sum()\n",
    "        #reset index to get planta as column\n",
    "        df_stational_factor_factory = df_stational_factor_factory.reset_index()\n",
    "        #convert columns to corresponding datatype for good measure.\n",
    "        df_stational_factor_factory['totalEntregado'] = df_stational_factor_factory['totalEntregado'].astype(float)\n",
    "        #Replace nan values with 0\n",
    "        df_stational_factor_factory['totalEntregado'] = df_stational_factor_factory['totalEntregado'].fillna(0)\n",
    "        #create stational factor column\n",
    "        df_stational_factor_factory['%FE_Factory'] = df_stational_factor_factory['totalEntregado']/df_stational_factor_factory['totalEntregado'].sum()\n",
    "        \n",
    "        #validate factories with weeks missing     \n",
    "        df_stational_factor_factory['vol_FE_factory'] = df_stational_factor_factory['%FE_Factory']*volume    \n",
    "\n",
    "    \n",
    "    if abs_type == 0:\n",
    "        #read excel with city volumes\n",
    "        vol_city = pd.read_excel('../abs_type/city/' + country + '.xlsx')\n",
    "        #get historic volume by city and factory\n",
    "        df_stational_factor_factory = df_history.groupby(['Ciudad','Planta'])['totalEntregado'].sum().reset_index()\n",
    "        #get historic data by city to then append to each factory of the corresponding city\n",
    "        df_city_history = df_history.groupby(['Ciudad'])['totalEntregado'].sum().reset_index()\n",
    "        #merge the 2 previous dataframes\n",
    "        df_stational_factor_factory = pd.merge(df_stational_factor_factory,\n",
    "                                               df_city_history[['Ciudad','totalEntregado']],on=\"Ciudad\",how=\"left\")  \n",
    "        #create stational factor column\n",
    "        df_stational_factor_factory['%FE_Factory'] = df_stational_factor_factory['totalEntregado_x']/df_stational_factor_factory['totalEntregado_y']\n",
    "        #merge with vol_city excel where the volumes that commercial gave us\n",
    "        df_stational_factor_factory = pd.merge(df_stational_factor_factory,vol_city,on=\"Ciudad\",how=\"left\")\n",
    "        #create our stational factor volume column\n",
    "        df_stational_factor_factory['vol_FE_factory'] = df_stational_factor_factory['%FE_Factory']*df_stational_factor_factory['vol_comercial']  \n",
    "        #drop unused columns\n",
    "        df_stational_factor_factory = df_stational_factor_factory.drop([\n",
    "            'Ciudad','totalEntregado_y','vol_comercial'\n",
    "        ], axis=1)\n",
    "        #rename column to eliminate _x\n",
    "        df_stational_factor_factory.rename(columns = {'totalEntregado_x':'totalEntregado'}, inplace = True)\n",
    "        #get list of every city name\n",
    "        city_list = vol_city['Ciudad'].to_list()\n",
    "        vol_list = vol_city['vol_comercial'].to_list()\n",
    "\n",
    "    \n",
    "    if abs_type == -1:\n",
    "        #read excel with city volumes\n",
    "        df_stational_factor_factory = pd.read_excel('../abs_type/factory/' + country + '.xlsx')\n",
    "        df_stational_factory_history = df_history.groupby(['Planta'])['totalEntregado'].sum().reset_index()\n",
    "        df_stational_factor_factory = pd.merge(df_stational_factor_factory,df_stational_factory_history, on=\"Planta\",how=\"left\")\n",
    "        df_stational_factor_factory.rename(columns = {'forecast_planta':'vol_FE_factory'}, inplace = True)\n",
    "        df_stational_factor_factory['%FE_Factory'] = 1\n",
    "        #get list of every factory\n",
    "        factory_list = df_stational_factor_factory['Planta'].to_list()\n",
    "        vol_list = df_stational_factor_factory['vol_FE_factory'].to_list()\n",
    "\n",
    "    \n",
    "    \n",
    "        #-------------------we will get the stational factors by week and factory-------------------\n",
    "    \n",
    " \n",
    "    #groupby week and factory with the sum of total volume\n",
    "    df_stational_factor_week = df_history.groupby(['Semana_Relativa','Planta'])['totalEntregado'].sum()\n",
    "\n",
    "    #reset index to get semana_relativa and planta as columns\n",
    "    df_stational_factor_week = df_stational_factor_week.reset_index()\n",
    "\n",
    "    #filter weeks in history dataset\n",
    "    df_stational_factor_week = df_stational_factor_week[df_stational_factor_week.Semana_Relativa.isin(calendar_sql.Semanas_mes)]\n",
    "    \n",
    "    #find weeks that are not present in historic data but are in the month to dissaggregate\n",
    "    week_not_present = list(set(calendar_sql['Semanas_mes'].unique()).symmetric_difference(set(df_stational_factor_week['Semana_Relativa'].unique())))\n",
    "\n",
    "    \n",
    "    \n",
    "    #convert columns to corresponding datatype for good measure.\n",
    "    df_stational_factor_week['totalEntregado'] = df_stational_factor_week['totalEntregado'].astype(float)\n",
    "    df_stational_factor_week['Semana_Relativa'] = df_stational_factor_week['Semana_Relativa'].astype(int)\n",
    "    #Replace nan values with 0\n",
    "    df_stational_factor_week['totalEntregado'] = df_stational_factor_week['totalEntregado'].fillna(0)\n",
    "    \n",
    "    #create dataframe to group volume totals by factory to then merge with the stational df\n",
    "    vol_por_planta_week = df_stational_factor_week.groupby('Planta')['totalEntregado'].sum()\n",
    "    #reset index to put factory as column\n",
    "    vol_por_planta_week = pd.DataFrame(vol_por_planta_week.reset_index())\n",
    "    #rename volumen column\n",
    "    vol_por_planta_week.rename(columns = {'totalEntregado':'vol_total_planta'}, inplace = True)\n",
    "    \n",
    "    #left merge to get total of each factory in corresponding row\n",
    "    df_stational_factor_week = pd.merge(df_stational_factor_week, \n",
    "                      vol_por_planta_week, \n",
    "                      on ='Planta', \n",
    "                      how ='left') \n",
    "    #create new column to get stational factor\n",
    "    df_stational_factor_week['%FE_week'] =  df_stational_factor_week['totalEntregado']/df_stational_factor_week['vol_total_planta']\n",
    "    df_stational_factor_week = df_stational_factor_week.sort_values(by=\"Planta\") #sort our dataframe by factory\n",
    "    \n",
    "    #we will create an extra dataframe for factories that dont have stational factories\n",
    "    df_week_general = df_stational_factor_week.groupby('Semana_Relativa')['totalEntregado'].sum().reset_index()\n",
    "    df_week_general['%FE_general'] = df_week_general['totalEntregado']/df_week_general['totalEntregado'].sum()\n",
    "\n",
    "    \n",
    "    #-------------we will basically repeat the previous process but for stational DAY factors-------------------\n",
    "    \n",
    "\n",
    "    df_stational_factor_day = df_history[df_history['DiaSemana'] != 1]\n",
    "    #groupby day and factory with the sum of total volume\n",
    "    df_stational_factor_day = df_stational_factor_day.groupby(['DiaSemana','Planta'])['totalEntregado'].sum()\n",
    "\n",
    "    #reset index to get semana_relativa and planta as columns\n",
    "    df_stational_factor_day = df_stational_factor_day.reset_index()\n",
    "    \n",
    "    #convert columns to corresponding datatype for good measure.\n",
    "    df_stational_factor_day['totalEntregado'] = df_stational_factor_day['totalEntregado'].astype(float)\n",
    "    df_stational_factor_day['DiaSemana'] = df_stational_factor_day['DiaSemana'].astype(int)\n",
    "    #Replace nan values with 0\n",
    "    df_stational_factor_day['totalEntregado'] = df_stational_factor_day['totalEntregado'].fillna(0)\n",
    "    \n",
    "    #create dataframe to group volume totals by factory to then merge with the stational df\n",
    "    vol_por_planta_day = df_stational_factor_day.groupby('Planta')['totalEntregado'].sum()\n",
    "    #reset index to put factory as column\n",
    "    vol_por_planta_day = pd.DataFrame(vol_por_planta_day.reset_index())\n",
    "    #rename volumen column\n",
    "    vol_por_planta_day.rename(columns = {'totalEntregado':'vol_total_planta'}, inplace = True)\n",
    "    \n",
    "    #left merge to get total of each factory in corresponding row\n",
    "    df_stational_factor_day = pd.merge(df_stational_factor_day, \n",
    "                      vol_por_planta_day, \n",
    "                      on ='Planta', \n",
    "                      how ='left')\n",
    "    \n",
    "    #add stational factor column\n",
    "    df_stational_factor_day['%FE_day'] =  df_stational_factor_day['totalEntregado']/df_stational_factor_day['vol_total_planta']\n",
    "    #sort the dataframe\n",
    "    df_stational_factor_day = df_stational_factor_day.sort_values([\"Planta\",\"DiaSemana\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #reorganize calendar to then merge for dissaggregation. Drop and rename columns\n",
    "    calendar_sql = calendar_sql.drop(['ID','Año','Mes','Días_Operativos_Acum','Semana_relativa','Total_Dias_Habiles_Mes'], axis=1)\n",
    "    calendar_sql.rename(columns = {'Dia_Semana':'DiaSemana'}, inplace = True)\n",
    "    calendar_sql.rename(columns = {'Semanas_mes':'Semana_Relativa'}, inplace = True)\n",
    "    calendar_sql['DiaSemana'] = calendar_sql['DiaSemana'].astype(int)\n",
    "    calendar_sql['Días_Operativos'] = calendar_sql['Días_Operativos'].astype(int)\n",
    "    #cross join to create structure for dissaggregation\n",
    "    df = pd.merge(calendar_sql,active_factories,how=\"cross\")\n",
    "    \n",
    "    #find holidays or sundays and convert Diasemana to 0. This will give these days a stational factor of 0.\n",
    "    df.loc[ df['Días_Operativos'] == 0, 'DiaSemana'] = 0\n",
    "    \n",
    "    #filter manual inactive factories if stated\n",
    "    if len(inactivate_temp) > 0:\n",
    "        df = df[~df.Centro.isin(inactivate_temp)]\n",
    "    #sort values\n",
    "    df = df.sort_values([\"Centro\",\"Fecha de entrega\"])\n",
    "    #rename columns for merge\n",
    "    df.rename(columns = {'Planta':'plantaunica'}, inplace = True)\n",
    "    df.rename(columns = {'Centro':'Planta'}, inplace = True)\n",
    "    \n",
    "    #merge with stational factory factors\n",
    "    df = pd.merge(df,df_stational_factor_factory,on=\"Planta\",how=\"left\")\n",
    "    #merge with stational week factors\n",
    "    #first validate if there is an uncommon week between historic dataframe and dissaggregation dataframe\n",
    "    if len(week_not_present) > 0:\n",
    "    #it should be close to impossible that there will be more than one week is missing\n",
    "    #print error just in case to capture those uncommon moments\n",
    "        if len(week_not_present) == 1:\n",
    "            week_missing = week_not_present[0]\n",
    "            df.loc[ df['Semana_Relativa'] == week_missing, 'Semana_Relativa'] = week_missing+1\n",
    "            print(\"week_missing: \"+str(week_missing))\n",
    "        else:\n",
    "            print(\"**error semanas**\")\n",
    "            return ['','','','']\n",
    "    else:\n",
    "        print(\"no weeks missing\")\n",
    "    #merge df with stational week factos\n",
    "    df = pd.merge(df,df_stational_factor_week,on=[\"Planta\",\"Semana_Relativa\"],how=\"left\")\n",
    "    #merge with stational day factors\n",
    "    df = pd.merge(df,df_stational_factor_day,on=[\"Planta\",\"DiaSemana\"],how=\"left\")\n",
    "    #drop unnecessary columns\n",
    "    df = df.drop(['totalEntregado_x','totalEntregado_y','vol_total_planta_x',\n",
    "                  'totalEntregado','vol_total_planta_y'], axis=1)    \n",
    "    \n",
    "    #fill nan values for week factors with 0\n",
    "    df['%FE_week'] = df['%FE_week'].fillna(0)\n",
    "    #merge df and df_wee_general to get general week stational factors\n",
    "    df = pd.merge(df,df_week_general[['Semana_Relativa','%FE_general']], on=\"Semana_Relativa\",how=\"left\")\n",
    "    #find stational factory week factors that are 0 and replace with general stational factor\n",
    "    df.loc[ df['%FE_week'] == 0, '%FE_week'] = df['%FE_general']\n",
    "    \n",
    "    #generate forecast column\n",
    "    df['vol_forecast'] = df['vol_FE_factory']*df['%FE_week']*df['%FE_day']\n",
    "    \n",
    "    #since the stational factors are not exactly the same as the historic data, there tends to always be a gap in the total vol.\n",
    "    #for this we will get the sum of that gap and repeat the process which at the end will add to the original forecast\n",
    "    #column. The while loop is used and stated to keep repeating until the gap is between 0 and 1.\n",
    "    \n",
    "    #iniciate a counter to create a new column for every gap found between the vol_forecast and original volumen\n",
    "    i = 1\n",
    "    \n",
    "    #while for abs_type 1 (general volume)\n",
    "    if abs_type == 1:\n",
    "        #check gap between total volumen and current forecast\n",
    "        vol_forecast_total = df['vol_forecast'].sum()\n",
    "        gap_total = volume - vol_forecast_total\n",
    "        print(\"gap original: \"+str(gap_total))\n",
    "        \n",
    "        #we want to decrease the gap_total to a max of 1. So we repeat the process of creating a new vol_forecast for every\n",
    "        #gap using the same stational factors until the gap is max 1. We sum 1 to i for the next while run\n",
    "\n",
    "        while gap_total < 0 or gap_total > 1: \n",
    "            df_stational_factor_factory['vol_FE_factory_gap_'+str(i)] = df_stational_factor_factory['%FE_Factory']*gap_total\n",
    "            df = pd.merge(df,df_stational_factor_factory[['Planta','vol_FE_factory_gap_'+str(i)]],on=\"Planta\",how=\"left\")\n",
    "            df['vol_forecast'] = df['vol_forecast'] + (df['vol_FE_factory_gap_'+str(i)]*df['%FE_week']*df['%FE_day'])\n",
    "            #check new gap\n",
    "            gap_total = volume - df['vol_forecast'].sum()\n",
    "            i = i +1\n",
    "    #abs_type = 0 for city level    \n",
    "    if abs_type == 0:\n",
    "        #create empty dataframe\n",
    "        df_1 = pd.DataFrame()\n",
    "        \n",
    "        for x, y in zip(city_list,vol_list):\n",
    "            df_test = df[df['Ciudad'] == x]\n",
    "            volume = y\n",
    "            #check gap between total volumen and current forecast\n",
    "            vol_forecast_total = df_test['vol_forecast'].sum()\n",
    "            gap_total = volume - vol_forecast_total\n",
    "\n",
    "\n",
    "            #we want to decrease the gap_total to a max of 1. So we repeat the process of creating a new vol_forecast for every\n",
    "            #gap using the same stational factors until the gap is max 1. We sum 1 to i for the next while run\n",
    "\n",
    "            while gap_total < 0 or gap_total > 0.1: \n",
    "                df_stational_factor_factory['vol_FE_factory_gap_'+str(i)] = df_stational_factor_factory['%FE_Factory']*gap_total\n",
    "                df_test = pd.merge(df_test,df_stational_factor_factory[['Planta','vol_FE_factory_gap_'+str(i)]],on=\"Planta\",how=\"left\")\n",
    "                df_test['vol_forecast'] = df_test['vol_forecast'] + (df_test['vol_FE_factory_gap_'+str(i)]*df_test['%FE_week']*df_test['%FE_day'])\n",
    "                #check new gap\n",
    "                gap_total = volume - df_test['vol_forecast'].sum()\n",
    "                i = i +1\n",
    "            #check if current city running is the same as the first in city list to get an empty dataframe at the start\n",
    "            if city_list[0] == x:\n",
    "                df_1 = df[0:0]\n",
    "            #after every while, append data to final dataset\n",
    "            df_1 = df_1.append(df_test,ignore_index=True)\n",
    "        #set the same variable name to maintain structure\n",
    "        df = df_1\n",
    "        \n",
    "    #abs_type = -1 for factory level    \n",
    "    if abs_type == -1:\n",
    "        #create empty dataframe\n",
    "        df_1 = pd.DataFrame()\n",
    "        \n",
    "        for x, y in zip(factory_list,vol_list):\n",
    "            df_test = df[df['Planta'] == x]\n",
    "            volume = y\n",
    "            #check gap between total volumen and current forecast\n",
    "            vol_forecast_total = df_test['vol_forecast'].sum()\n",
    "            gap_total = volume - vol_forecast_total\n",
    "\n",
    "\n",
    "            #we want to decrease the gap_total to a max of 1. So we repeat the process of creating a new vol_forecast for every\n",
    "            #gap using the same stational factors until the gap is max 1. We sum 1 to i for the next while run\n",
    "\n",
    "            while gap_total < 0 or gap_total > 0.1: \n",
    "                df_stational_factor_factory['vol_FE_factory_gap_'+str(i)] = df_stational_factor_factory['%FE_Factory']*gap_total\n",
    "                df_test = pd.merge(df_test,df_stational_factor_factory[['Planta','vol_FE_factory_gap_'+str(i)]],on=\"Planta\",how=\"left\")\n",
    "                df_test['vol_forecast'] = df_test['vol_forecast'] + (df_test['vol_FE_factory_gap_'+str(i)]*df_test['%FE_week']*df_test['%FE_day'])\n",
    "                #check new gap\n",
    "                gap_total = volume - df_test['vol_forecast'].sum()\n",
    "                i = i +1\n",
    "\n",
    "            #check if current city running is the same as the first in city list to get an empty dataframe at the start\n",
    "            if factory_list[0] == x:\n",
    "                df_1 = df[0:0]\n",
    "            #after every while, append data to final dataset\n",
    "            df_1 = df_1.append(df_test,ignore_index=True)\n",
    "        #set the same variable name to maintain structure\n",
    "        df = df_1\n",
    "        \n",
    "    #after the while run is complete, we must clean the final forecast dataframe by dropping unnecessary colums\n",
    "    #use a list and regex filter to drop columns the repeat same specific string\n",
    "    df = df[df.columns.drop(list(df.filter(regex='vol_FE_factory_gap')))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex='vol_FE_factory')))]\n",
    "    df_stational_factor_factory = df_stational_factor_factory[df_stational_factor_factory.columns.drop(list(df_stational_factor_factory.filter(regex='vol_FE_factory')))]\n",
    "    df = df.drop(['%FE_week','%FE_day','DiaSemana','Cluster'], axis=1) \n",
    "    #fill nan values with 0\n",
    "    df['vol_forecast'] = df['vol_forecast'].fillna(0)\n",
    "    #reindex to shape the columns to sql structure\n",
    "    df = df.reindex(columns=['pais','Ciudad','Planta','plantaunica','Fecha de entrega','vol_forecast'])\n",
    "        \n",
    "    print(\"exitoso\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51242d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Colombia' #state the country\n",
    "start_date_history = datetime.datetime(2019, 1, 1) #the start date for our model to analyze\n",
    "end_date_history = datetime.datetime(2022, 12 , 31) #the end date for our model to analyze\n",
    "df_history = querySQL(  \"{CALL SCAC_AP20_BaseDesagregacionV2 (?,?,?)}\", (country, start_date_history.strftime(\"%Y-%m-%d\"), end_date_history.strftime(\"%Y-%m-%d\") ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79441e23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barranquilla', 'Bogotá', 'Bucaramanga', 'Cali', 'Cartagena', 'Cúcuta', 'Fusagasuga', 'Ibagué', 'Maceo', 'Medellín', 'Neiva', 'Paraiso Central', 'Pereira', 'Ricaurte', 'Rionegro', 'Santa Marta', 'TESALIA', 'Tuluá']\n",
      "[5 6 7 8 9]\n",
      "[5 6 7 8 9]\n",
      "[]\n",
      "   Semana_Relativa  totalEntregado  %FE_general\n",
      "0                5       130717.50     0.195366\n",
      "1                6       133165.25     0.199025\n",
      "2                7       137487.00     0.205484\n",
      "3                8       135997.25     0.203257\n",
      "4                9       131722.40     0.196868\n",
      "no weeks missing\n",
      "exitoso\n"
     ]
    }
   ],
   "source": [
    "year_disaggregate = 2023 # year to disaggregate \n",
    "month_disaggregate = 2\n",
    "volume = 0 #only for abs_type = 1\n",
    "inactivate_temp = ['T001','T002','T003','T004'] #place inactive factories if there are any\n",
    "abs_type = 0\n",
    "\"\"\"\n",
    "PARAMETROS:\n",
    "absorcionEstadistica = 1  -> get general volume\n",
    "absorcionEstadistica = 0  -> get volume by city\n",
    "absorcionEstadistica = -1 -> get volume by factory\n",
    "\n",
    "\"\"\"\n",
    "#get weeks corresponding to the desired month\n",
    "calendar_sql = querySQL( \"select * from SCAC_AT3_DiasHabilesFuente where pais = ? and año = ? and mes = ? order by [Fecha de entrega]\", (country,year_disaggregate,month_disaggregate) )\n",
    "#get active factories to filter out inactive\n",
    "active_factories = querySQL( \"select Centro, [Planta Unica] as Planta, [Desc Cluster] as Cluster, Ciudad_Cluster as Ciudad  from SCAC_AT1_NombreCluster where pais = ? and activo = 1 order by Centro\", (country) )\n",
    "#execute dissagregation\n",
    "df_pivot_stational_factor = get_stational_factors(df_history,country,year_disaggregate,month_disaggregate,inactivate_temp,calendar_sql,volume,abs_type,active_factories)\n",
    "send_excel(df_pivot_stational_factor,country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fe8f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b489379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e704407f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df7fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109877b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82391c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
